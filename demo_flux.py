import torch
import json
from PIL import Image
import numpy as np
import gradio as gr
import os

from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path

from third_party.flux.xflux_pipeline import XFluxPipeline

from src.resampler import Resampler_cross
from src.utils import get_random_instruction
from src.run_llava_gen import eval_model

ckpts = "./ckpts"
with open("assets/edit_instructions.json", 'r') as file:
    instructions = json.load(file)
    
weight_dtype = torch.float16
# try different devices for mllm and diffusion when facing oom issues
lm_device = "cuda:0"
flux_device = "cuda:0"

model_path = os.path.join(ckpts, "llava")
lm_tokenizer, lm_model, lm_image_processor, lm_context_len = load_pretrained_model(
    model_path=model_path,
    model_base=None,
    model_name=get_model_name_from_path(model_path),
    device_map=lm_device
)
lm_model = lm_model.to(lm_device, dtype=torch.bfloat16)
lm_model.eval()

ip_repo_id = "XLabs-AI/flux-ip-adapter"
ip_name = "ip_adapter.safetensors"
model_type = "flux-dev"
ip_local_path = None
offload = False
xflux_pipeline = XFluxPipeline(model_type, flux_device, offload)
xflux_pipeline.set_ip(ip_local_path, ip_repo_id, ip_name)

aligner  = Resampler_cross(dim=4096, depth=4, dim_head=64, heads=20, num_queries=16, embedding_dim=5120, output_dim=4096, ff_mult=4,)
aligner = aligner.to(lm_device, dtype=torch.float32)
aligner.eval()

flux_model_ckpt = os.path.join(ckpts, 'aligner_flux.pth')
flux_model_state_dict = torch.load(flux_model_ckpt, map_location='cpu')
flux_new_state_dict = {k[7:] if k.startswith('module.') else k: v for k, v in flux_model_state_dict.items()}
aligner.load_state_dict(flux_new_state_dict)

width = 1024
height = 1024
neg_image_prompt = np.zeros((width, height, 3), dtype=np.uint8)
neg_image_proj = xflux_pipeline.get_image_proj(neg_image_prompt)


def generate_image(text_prompt, seed):

    flux_image = xflux_pipeline.forward(
        prompt=text_prompt,
        width=width,
        height=height,
        guidance=3.5,
        num_steps=30,
        seed=seed,
        controlnet_image=None,
        timestep_to_start_cfg=1,
        true_gs=3.5,
        control_weight=0.8,
        neg_prompt="",
        image_proj=neg_image_proj,
        neg_image_proj=neg_image_proj,
        ip_scale=0.0,
        neg_ip_scale=0.0,
    )

    ip_feat = xflux_pipeline.get_image_proj(flux_image).to(device=lm_device)

    random_instruction = get_random_instruction(instructions)
    prompt = random_instruction.replace("{prompt}", text_prompt)
    lm_args = type('Args', (), {
        "model_path": model_path,
        "model_base": None,
        "model_name": get_model_name_from_path(model_path),
        "query": prompt,
        "conv_mode": None,
        "image_file": [flux_image],
        "sep": ",",
        "temperature": 0,
        "top_p": None,
        "num_beams": 1,
        "max_new_tokens": 512
    })()

    with torch.no_grad():
        hidden_states, _ = eval_model(lm_args, lm_tokenizer, lm_model, lm_image_processor, lm_context_len, dtype=torch.bfloat16, generate=False, layer=-1)
        hidden_states = hidden_states.to(dtype=torch.float32)
        ip_feat = ip_feat.to(dtype=torch.float32)
        for _ in range(3):
            ip_feat = aligner(hidden_states, ip_feat)

    aligned_ip_feat = ip_feat.to(device=flux_device, dtype=torch.bfloat16)

    generated_image = xflux_pipeline.forward(
        prompt=text_prompt,
        width=width,
        height=height,
        guidance=3.5,
        num_steps=30,
        seed=seed,
        controlnet_image=None,
        timestep_to_start_cfg=1,
        true_gs=3.5,
        control_weight=0.8,
        neg_prompt="",
        image_proj=aligned_ip_feat,
        neg_image_proj=neg_image_proj,
        ip_scale=0.2,
        neg_ip_scale=0.2,
    )

    return flux_image, generated_image

with gr.Blocks() as demo:
    with gr.Row():
        prompt = gr.Textbox(label="Prompt", value="One morning I chased an elephant in my pajamas")
        seed = gr.Number(label="Seed", value=100)
    with gr.Row():
        output_flux = gr.Image(label="Image Generated by FLUX", type="pil")
        output_img = gr.Image(label="Image Re-generated by IMG (Ours)", type="pil")
    generate_btn = gr.Button("Generate")

    gr.Examples(
        examples=[
            ["One morning I chased an elephant in my pajamas", 100],
            ["a woman on the top of a butterfly", 2],
            ["a rubber ball and a metallic car", 2],
            ["A whimsical scene featuring a small elf with pointed ears and a green hat, sipping orange juice through a long straw from a disproportionately large orange. Next to the elf, a curious squirrel perches on its hind legs, while an owl with wide, observant eyes watches intently from a branch overhead. The orange's vibrant color contrasts with the muted browns and greens of the surrounding forest foliage.", 100]
            ],
        inputs=[prompt, seed],
        outputs=[output_flux, output_img],
        fn=generate_image,
    )

    generate_btn.click(
        fn=generate_image,
        inputs=[prompt, seed],
        outputs=[output_flux, output_img]
    )

demo.launch(server_name="0.0.0.0", 
        server_port=443,
        share=False, 
        inbrowser=False)